<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Accord.Neuro</name>
    </assembly>
    <members>
        <member name="T:Accord.Neuro.NguyenWidrowInitializer">
            <summary>
             Nguyen-Widrow weight initializer
            </summary>
            <remarks>
            <para>The Nguyen-Widrow initialization algorithm chooses values in
            order to distribute the active region of each neuron in the layer
            approximately evenly across the layers' input space.</para>
            
            <para>The values contain a degree of randomness, so they are not the
            same each time this function is called.</para> 
            </remarks>
            
        </member>
        <member name="M:Accord.Neuro.NguyenWidrowInitializer.#ctor(AForge.Neuro.ActivationNetwork)">
            <summary>
              Constructs a new Nguyen-Widrow Weight Initializer.
            </summary>
            <param name="network">The activation network whose weights will be initialized.</param>
        </member>
        <member name="M:Accord.Neuro.NguyenWidrowInitializer.Randomize">
            <summary>
              Randomizes (initializes) the weights of
              the network using Nguyen-Widrow method's.
            </summary>
        </member>
        <member name="T:Accord.Neuro.Learning.JacobianMethod">
            <summary>
              The Jacobian computation method used by the Levenberg-Marquardt.
            </summary>
        </member>
        <member name="F:Accord.Neuro.Learning.JacobianMethod.ByFiniteDifferences">
            <summary>
              Computes the Jacobian using approximation by finite differences. This
              method is slow in comparison with back-propagation and should be used
              only for debugging or comparison purposes.
            </summary>
        </member>
        <member name="F:Accord.Neuro.Learning.JacobianMethod.ByBackpropagation">
            <summary>
              Computes the Jacobian using back-propagation for the chain rule of
              calculus. This is the preferred way of computing the Jacobian.
            </summary>
        </member>
        <member name="T:Accord.Neuro.Learning.LevenbergMarquardtLearning">
            <summary>
              Levenberg-Marquardt Learning Algorithm with optional Bayesian Regularization.
            </summary>
            
            <remarks>
            <para>This class implements the Levenberg-Marquardt learning algorithm,
            which treats the neural network learning as a function optimization
            problem. The Levenberg-Marquardt is one of the fastest and accurate
            learning algorithms for small to medium sized networks.</para>
            
            <para>However, in general, the standard LM algorithm does not perform as well
            on pattern recognition problems as it does on function approximation problems.
            The LM algorithm is designed for least squares problems that are approximately
            linear. Because the output neurons in pattern recognition problems are generally
            saturated, it will not be operating in the linear region.</para>
            
            <para>The advantages of the LM algorithm decreases as the number of network
            parameters increases. </para>
            
            <para>Sample usage (training network to calculate XOR function):</para>
            <code>
            // initialize input and output values
            double[][] input = new double[4][] {
                new double[] {0, 0}, new double[] {0, 1},
                new double[] {1, 0}, new double[] {1, 1}
            };
            double[][] output = new double[4][] {
                new double[] {0}, new double[] {1},
                new double[] {1}, new double[] {0}
            };
            // create neural network
            ActivationNetwork   network = new ActivationNetwork(
                SigmoidFunction( 2 ),
                2, // two inputs in the network
                2, // two neurons in the first layer
                1 ); // one neuron in the second layer
            // create teacher
            LevenbergMarquardtLearning teacher = new LevenbergMarquardtLearning( network );
            // loop
            while ( !needToStop )
            {
                // run epoch of learning procedure
                double error = teacher.RunEpoch( input, output );
                // check error value to see if we need to stop
                // ...
            }
            </code>
            
            <para>
              References:
              <list type="bullet">
                <item><description><a href="http://www.cs.otago.ac.nz/nnweb/FAQ2.html">
                  http://www.cs.otago.ac.nz/nnweb/FAQ2.html</a></description></item>
                <item><description><a href="http://www-alg.ist.hokudai.ac.jp/~jan/alpha.pdf">
                  http://www-alg.ist.hokudai.ac.jp/~jan/alpha.pdf</a></description></item>
                <item><description><a href="http://cs.olemiss.edu/~ychen/publications/conference/chen_ijcnn99.pdf">
                  http://cs.olemiss.edu/~ychen/publications/conference/chen_ijcnn99.pdf</a></description></item>
                <item><description><a href="http://www.nict.go.jp/publication/shuppan/kihou-journal/journal-vol54no1.2/02D.pdf">
                  http://www.nict.go.jp/publication/shuppan/kihou-journal/journal-vol54no1.2/02D.pdf</a></description></item>
                <item><description><a href="http://matlab.izmiran.ru/help/toolbox/nnet/backpr12.html">
                  http://matlab.izmiran.ru/help/toolbox/nnet/backpr12.html</a></description></item>
                <item><description><a href="http://www.inference.phy.cam.ac.uk/mackay/Bayes_FAQ.html">
                  http://www.inference.phy.cam.ac.uk/mackay/Bayes_FAQ.html</a></description></item>
                <item><description><a href="http://eprints.kfupm.edu.sa/40648/1/40648.pdf">
                  http://eprints.kfupm.edu.sa/40648/1/40648.pdf</a></description></item>
              </list>
            </para>   
            </remarks>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.#ctor(AForge.Neuro.ActivationNetwork)">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.Neuro.Learning.LevenbergMarquardtLearning"/> class.
            </summary>
            
            <param name="network">Network to teach.</param>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.#ctor(AForge.Neuro.ActivationNetwork,System.Boolean)">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.Neuro.Learning.LevenbergMarquardtLearning"/> class.
            </summary>
            
            <param name="network">Network to teach.</param>
            <param name="useRegularization">True to use bayesian regularization, false otherwise.</param>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.#ctor(AForge.Neuro.ActivationNetwork,System.Boolean,Accord.Neuro.Learning.JacobianMethod)">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.Neuro.Learning.LevenbergMarquardtLearning"/> class.
            </summary>
            
            <param name="network">Network to teach.</param>
            <param name="useRegularization">True to use bayesian regularization, false otherwise.</param>
            <param name="method">The method by which the Jacobian matrix will be calculated.</param>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.Run(System.Double[],System.Double[])">
             <summary>
              This method should not be called. Use <see cref="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.RunEpoch(System.Double[][],System.Double[][])"/> instead.
             </summary>
             
             <param name="input">Array of input vectors.</param>
             <param name="output">Array of output vectors.</param>
             
             <returns>Nothing.</returns>
             
             <remarks><para>Online learning mode is not supported by the
             Levenberg Marquardt. Use batch learning mode instead.</para></remarks>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.RunEpoch(System.Double[][],System.Double[][])">
             <summary>
               Runs a single learning epoch.
             </summary>
             
             <param name="input">Array of input vectors.</param>
             <param name="output">Array of output vectors.</param>
             
             <returns>Returns summary learning error for the epoch.</returns>
             
             <remarks><para>The method runs one learning epoch, by calling running necessary
             iterations of the Levenberg Marquardt to achieve an error decrease.</para></remarks>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.CalculateError(System.Double[])">
            <summary>
              Calculates error values for the last network layer.
            </summary>
            
            <param name="desiredOutput">Desired output vector.</param>
            
            <returns>Returns summary squared error of the last layer divided by 2.</returns>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.UpdateWeights">
            <summary>
             Update network's weights.
            </summary>
            
            <returns>The sum of squared weights divided by 2.</returns>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.CreateWeights">
            <summary>
              Creates the initial weight vector w
            </summary>
            
            <returns>The sum of squared weights divided by 2.</returns>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.getNumberOfParameters(AForge.Neuro.ActivationNetwork)">
            <summary>
              Gets the number of parameters in a network.
            </summary>
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.JacobianByChainRule(System.Double[][],System.Double[][])">
            <summary>
              Calculates the Jacobian matrix by using the chain rule.
            </summary>
            <param name="input">The input vectors.</param>
            <param name="output">The desired output vectors.</param>
            <returns>The sum of squared errors for the last error divided by 2.</returns>
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.CalculateDerivatives(System.Double[],System.Double[],System.Int32)">
            <summary>
              Calculates partial derivatives for all weights of the network.
            </summary>
            
            <param name="input">The input vector.</param>
            <param name="desiredOutput">Desired output vector.</param>
            <param name="outputIndex">The current output location (index) in the desired output vector.</param>
            
            <returns>Returns summary squared error of the last layer.</returns>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.JacobianByFiniteDifference(System.Double[][],System.Double[][])">
            <summary>
              Calculates the Jacobian Matrix using Finite Differences
            </summary>
            <returns>Returns the sum of squared errors of the network divided by 2.</returns>
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.createCoefficients(System.Int32)">
            <summary>
              Creates the coefficients to be used when calculating
              the approximate Jacobian by using finite differences.
            </summary>
            
        </member>
        <member name="M:Accord.Neuro.Learning.LevenbergMarquardtLearning.ComputeDerivative(System.Double[],System.Int32,System.Int32,System.Int32,System.Double@,System.Double)">
            <summary>
              Computes the derivative of the network in respect to the
              weight passed as parameter.
            </summary>
        </member>
        <member name="P:Accord.Neuro.Learning.LevenbergMarquardtLearning.LearningRate">
             <summary>
              Learning rate
             </summary>
             
             <remarks><para>The value determines speed of learning.</para>
             
             <para>Default value equals to <b>0.1</b>.</para>
             </remarks>
            
        </member>
        <member name="P:Accord.Neuro.Learning.LevenbergMarquardtLearning.Adjustment">
             <summary>
             Learning rate adjustment
             </summary>
             
             <remarks><para>The value by which the learning rate
             is adjusted when searching for the minimum cost surface.</para>
             
             <para>Default value equals to <b>10</b>.</para>
             </remarks>
            
        </member>
        <member name="P:Accord.Neuro.Learning.LevenbergMarquardtLearning.NumberOfParameters">
            <summary>
              Gets the total number of parameters in the network
              being teached.
            </summary>
        </member>
        <member name="P:Accord.Neuro.Learning.LevenbergMarquardtLearning.EffectiveParameters">
            <summary>
              Gets the number of effective parameters being used
              by the network as determined by the bayesian regularization.
            </summary>
            <remarks>
              If no regularization is being used, the value will be 0.
            </remarks>
        </member>
        <member name="P:Accord.Neuro.Learning.LevenbergMarquardtLearning.Alpha">
            <summary>
              Gets or sets the importance of the squared sum of network
              weights in the cost function. Used by the regularization.
            </summary>
            <remarks>
              This is the first bayesian hyperparameter. The default
              value is 0.
            </remarks>
        </member>
        <member name="P:Accord.Neuro.Learning.LevenbergMarquardtLearning.Beta">
            <summary>
              Gets or sets the importance of the squared sum of network
              errors in the cost function. Used by the regularization.
            </summary>
            <remarks>
              This is the second bayesian hyperparameter. The default
              value is 1.
            </remarks>
        </member>
        <member name="P:Accord.Neuro.Learning.LevenbergMarquardtLearning.UseRegularization">
            <summary>
              Gets or sets whether to use Bayesian Regularization.
            </summary>
        </member>
    </members>
</doc>
